{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST Recurrent Neural Network",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matt594/Intro-to-Deep-Learning/blob/master/MNIST_Recurrent_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Ae26f2A54jlM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Interpretting a Recurrent Neural Network**\n",
        "\n",
        "This notebook is a breakdown of recurrent neural networks with Keras. The goal of this notebook is to provide a compact reference for those with an entirely fresh background to computer science and/or machine learning. This is also just one out of three notebooks; the other two notebooks tackle multi-layer perceptrons (which I recommend viewing first) and convolutional neural networks.\n",
        "\n",
        "A recurrent neural network is a deep learning structure commonly used to predict trends in data where the sequence of information is important for it's interpretation like texts, videos, and stocks. When data is fed into a recurrent neural network, it produces an output that is fed back into the neural network with new input. This lets the network directly evaluate new data based off of past experiences and discover patterns. Thus, recurrent neural networks are incredible tools for predicting stock trends, speech recognition, and translating languages. It's also amusing to let recurrent neural networks write their own stories and compose their own music with their predictive power. The recurrent neural network in this notebook will focus on natural language processing which means the network will analyze textual information. More specifically, our network will classify movie reviews as either positive or negative.\n",
        "\n",
        "In each cell, I will attempt to provide a grounds-up explanation of the cell's contents, so the cell explanations may stray from machine learning concepts time to time and repeat content provided by the two other notebooks. Again, the goal of this notebook is to provide a reference for anyone to understand while remaining compact. If there's a topic you'd like to look into more, I've provided some extra sources of information in each cell.\n",
        "\n",
        "Description of code cells should include:\n",
        "1. Title: What topic(s) are being covered in the cell\n",
        "2. Purpose: Goal of the cell\n",
        "3. Execution: What the code is doing\n",
        "4. Vocabulary/Concepts: Terminology and concepts for computer science of machine learning\n",
        "5. General: Common principle/practice regarding cell operations and code\n",
        "6. Interpretating Code: A guide to deciphering the cell's code\n",
        "7. External Information: Sources for more information\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "rX73-4J0tgeZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Import necessary libraries\n",
        "import keras\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZA0DcwD95ZIg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Import Statements**\n",
        "\n",
        "This first cell imports modules to make the coding process simpler. With pre-made modules, the programmer doesn't have to reinvent the wheel. They can spend more time creating an applicable program and less time creating a modular foundation.\n",
        "\n",
        "**Vocabulary/Concepts**\n",
        "*   Importing...Makes data accessible from another module\n",
        "*   Module.......Collection of pre-made methods, classes, or programs\n",
        "\n",
        "**Execution**\n",
        "\n",
        "When a program calls for an import, the program searches for a module of the requested name on a system path.Though, before importing, modules must be downloaded or created so they have an address on the machine where the program is running. Once again, these are crucial because they'll save us time when creating our network; we won't need to create one from scratch.\n",
        "\n",
        "**General** \n",
        "*   When importing, distinguish what modules must be used. No program needs to know every module on your machine.\n",
        "*   Chunk imports together that are related to each other.\n",
        "*   When importing created modules, like classes, it's best to keep all of the program's modules in one file so you don't need to change the system path.\n",
        "*   Not every module is supported over multiple programming languages.\n",
        "*   If you're having trouble with a particular task, look up modules for your programming languageâ€”you may find something incredibly useful to import.\n",
        "\n",
        "**Interpretting Code**\n",
        "*   \"import\" calls for modules to import\n",
        "*   \"from\" specifies what should be imported from a module rather than importing the entire module\n",
        "*   \"as\" changes how the module is called when it's used in the program\n",
        "*   \"keras\" is a machine learning module for python that simplifies Google's Tensorflow\n",
        "*   \"numpy\" is an array manipulation module for python\n",
        "*   \"matplotlib\" is a data plotting module\n",
        "*   \"os\" is a python module for your machine's operating system\n",
        "\n",
        "**External Information**\n",
        "*   [Python Import System](https://docs.python.org/3/reference/import.html)\n",
        "*   [Keras Documentation](https://keras.io)\n",
        "*   [Numpy Documentation](http://www.numpy.org)\n",
        "*   [Matplotlib Documentation](https://matplotlib.org)\n",
        "*   [Python Documentation](https://www.python.org)\n",
        "\n",
        "These sources are great for interpretting what modules are being used in this notebook. If there is any uncertainty as to what operation is being done, it never hurts to look up the operation at the documentation's source.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "TbDQFxiA4j4k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Suppress Warnings (Optional)\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qTjlfMlR4kD3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Variable Initialization & Environmental Variables**\n",
        "\n",
        "This cell suppresses warnings and disables debugging logs from Tensorflow. This is mostly just for preference since the program can operate without this line of code, but it makes the program notifications less annoying.\n",
        "\n",
        "**Vocabulary/Concepts**\n",
        "*   Variable Initialization.......Assigns a value to a variable\n",
        "  *   Variable............................Stores data under a name and type\n",
        "  *   Data..................................Information\n",
        "  *   Data type..........................Form data takes to represent itself which includes integers, floats, strings, etc.\n",
        "*   Environmental Variable...A value that affects running process behavior\n",
        "\n",
        "**Execution**\n",
        "\n",
        "Variable initilization sets one variable equal to a value. In this case, an environmental variable is being set to the value \"2\" which will be relayed to Tensorflow in order to prevent debugging and warnings from being displayed. Again, this statement was included as a preference since the notifications can be distracting, but the network can operate without suppressing Tensorflow's warnings.\n",
        "\n",
        "**General**\n",
        "*   Environmental variables can alter the way your program runs so it is important to be aware of how each variable affects the program when coding. \n",
        "*   Variable initialization is a basic computer science operation. It is crucial in almost every program. \n",
        "\n",
        "**Interpretting Code**\n",
        "*   \"os\" refers to the python operating system module\n",
        "*   \"environ['TF_CPP_MIN_LOG_LEVEL']\" specifies the environmental variable being changed\n",
        "*   \"=\" is an operator where the variable on the left side is set equal to the value on the right\n",
        "*   \"2\" is the value that the environmental variable is set to which is one of the modes for tensorflow logging\n",
        "\n",
        "**External Information**\n",
        "*   [Python Operators](https://www.tutorialspoint.com/python/python_basic_operators.htm)\n",
        "*   [Variable Intialization](https://stackoverflow.com/questions/23345554/the-differences-between-initialize-define-declare-a-variable)\n",
        "*   [Environmental Variables](https://en.wikipedia.org/wiki/Environment_variable)\n",
        "*   [Operating System Documentation](https://docs.python.org/2/library/os.html)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ZjL-vO_MGvDg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Set Variables\n",
        "epochs = 3\n",
        "batch_size = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W29cldnBGwKx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **More Variable Initialization & ML Variables**\n",
        "\n",
        "This cell initializes more variables that will be used for training and classification.\n",
        "\n",
        "**Vocabulary/Concepts**\n",
        "*   Epochs.................................Times a network runs through an entire dataset\n",
        "*   Batches................................A group of training examples. The network can only update after training through all the examples in the batch. Important information from each example is saved for updating until the batch is completed\n",
        "  *   Update..................................Synonym for backpropagate. Backpropagation occurs after each batch during training and will be discussed in the next cell\n",
        "\n",
        "**Execution**\n",
        "\n",
        "Each line of code is initializing a separate variable which all take the form of integers. These variables will be used later to instruct the neural network how long it should run for and how many examples it should update after.\n",
        "\n",
        "**General**\n",
        "*   You may wonder \"why initialize a variable if it's just a number that can be hard-coded?\" This highlights two more important aspect of variable initialization: labeling and multiple usage.\n",
        "  *   Labeling numbers gives others a better understanding of what is being used. For example, a method that uses \"epochs\" as an input tells the reader more than a method that uses \"50\" as an input.\n",
        "  *   Using a value multiple times without assigning it to a variable can lead future problems. For example, if you use \"50\" everywhere and you want to change all 50s to 40s, you'll have manually change every occurence of 50. If you assign 50 to \"epochs\", you can change the variable's value without manually finding every instance of the variable.\n",
        "*   Group your initialized variables together; they'll be easier to find if you need to change their values.\n",
        "\n",
        "**Interpretting Code**\n",
        "*   \"epochs\" is set to 3. This means the network will run through the entire training dataset 3 times\n",
        "*   \"batch_size\" is set to 64. This means the network will run through 64 moview reviews before updating\n",
        "\n",
        "**External Information**\n",
        "*   [Epochs, Batches, & Iterations](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "VL_jc-wQ4kQQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Load and separate data into X: Features, Y: Labels\n",
        "top_words = 5000 #Loads top 5000 words\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "print(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zF-8rzzG4kYv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Training Data & Testing Data, Plus Methods**\n",
        "\n",
        "This cell initializes the imdb data into training data with training labels and test data with test labels which will later be interpretted by the neural network.\n",
        "\n",
        "**Vocabulary/Concepts**\n",
        "*   Methods......................Operations under a name that accomplish a task\n",
        "*   Data.............................Information. In this case, the data are words\n",
        "*   Labels..........................The identification of data\n",
        "*   Tuples, Arrays, Lists...Series of data\n",
        "*   Training data...............Data the algorithm evaluates to backpropagate and improve\n",
        "*   Test data.....................Data the algorithm evaluates so people can measure the algorithms's performance\n",
        "*   Training.......................Trial runs on training data to fuel backpropagation\n",
        "  *   Backpropagation......Mathematical analysis of loss to reduce future loss. Backpropagation occurs after a batch is completed. It is the source of a network's \"learning\" capabilities\n",
        "  *   Loss...........................Also known as error. Measures how far off the network's classification is from the true value\n",
        "*   Testing........................Occurs after training to measure how well the machine has learned. Testing is a countermeasure to overfitting\n",
        "  *   Overfitting.................Dilemma that occurs when an algorithm gets extremely good at classifying training data, but performs worse when classifying new data. Essentially, the machine has memorized the training data and ignores the patterns that can apply to new data\n",
        "  \n",
        "**Execution**\n",
        "\n",
        "The first line initializes the variable \"top_words\" to 5000. Then, the \"load_data()\" method from keras takes \"numwords=top_words\" as a parameter to return a tuple of tuples. These tuples will be initialized as a training data and training label tuple and a test data and test label tuple respectively. This split provides data for both training and testing of the neural network. However, the data is still not ready to be used.\n",
        "\n",
        "**General**\n",
        "*   Methods and objects can be created by the programmer or can come from imported modules. They can be applied to class objects or can be applied using the import calls. Methods also often require parameters (which are placed between the parentheses) and can return a value. In this case, the \".load_data()\" method returns a tuple of tuples.\n",
        "*   There are three types of data in series:\n",
        "  *   Tuples are denoted by parentheses, are limited in size, can carry different objects, and cannot be edited once created. \n",
        "  *   Lists are denoted by brackets, are unlimited in size, can carry different objects, and can be edited once created. \n",
        "  *   Arrays are denoted by brackets and are either created with the Python array module or the Numpy module. They are unlimited in size, carry one type of object, and can be edited once created. Additionally, they're valuable since they can undergo manipulation that affects the entire array.\n",
        "*   Keep in mind that the elements in the tuples inside the tuple refer to an array of elements that represent words.\n",
        "*   It is important to note that arrays, lists, and tuples follow a specific rule when they are indexed. For instance, let's assume \"arr\" is an array of 10 integers in sequence from 1 to 10.  arr[0] refers to the first integer in the array: 1. arr[1] refers to the second integer: 2. At the very end, arr[9] refers to 10. arr[10] would throw an error because it is out of bounds. Thus, when indexing or creating one of these three objects, always keep in mind that the index starts counting at [0] and ends at [(range) - 1].\n",
        "*   Training data should be around twice as large as test data.\n",
        "*   Don't let your machine train for too many epochs otherwise you risk overfitting\n",
        "*   To gauge when your algorithm begins to overfit, keep track of metrics by printing out information as the machine trains\n",
        "*  \"x\" in machine learning always refers to the input data.\n",
        "*  \"y\" in machine leanring always refers to the labels.\n",
        "\n",
        "**Interpretting Code**\n",
        "*   \"top_words\" is set to 5000. This means that the neural network's will only look at the first 5000 words of the dataset\n",
        "*   To return the split data from imdb, \"load_data()\" is taking one parameter:\n",
        "  *   \"num_word=top_words\" refers to how many words \"load_data()\" should take from the dataset. In this case, it takes the top 5000 words\n",
        "*   \"(x_train, y_train), (x_test, y_test)\" are tuples set equal to the tuples returned by the \"load_data()\" method\n",
        "\n",
        "**External Information**\n",
        "*   [Lists, Arrays, and Tuples](https://stackoverflow.com/questions/626759/whats-the-difference-between-lists-and-tuples)\n",
        "*   [Array Indexing](https://en.wikipedia.org/wiki/Array_data_structure#Element_identifier_and_addressing_formulas)\n",
        "*   [Methods](https://en.wikipedia.org/wiki/Method_(computer_programming)\n",
        "*   [Training and Test Data](https://www.quora.com/What-is-a-training-data-set-test-data-set-in-machine-learning-What-are-the-rules-for-selecting-them)\n",
        "*   [Backpropagation](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)\n",
        "*   [Overfitting](https://elitedatascience.com/overfitting-in-machine-learning)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "wDcDOluK4khh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VxCaRAxF4kpx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Data Formatting**\n",
        "\n",
        "This cell executes a function called \"padding\" on the input data to ensure that all data is uniform in length. This is imperative for an operational neural network because input data must match the number of neurons in the first layer. The structure of this recurrent neural network will be covered later.\n",
        "\n",
        "**Vocabulary/Concepts**\n",
        "*   Padding.......Adding elements to sequences to make every sequence the same length\n",
        "*   Truncating...Subtracting elements from sequences to make every sequence the same length\n",
        "\n",
        "**Execution**\n",
        "\n",
        "The first line initializes the variable \"max_review_length\" to 500. The next two lines reinitialize \"X_train\" and \"X_test\" elements into sequences with the same lengths since each element in \"X_train\" and \"X_test\" are are sentences. Again, this is critical for inputting data into the neural network correctly. Otherwise, the network may not function or may interpret data erroneously.\n",
        "\n",
        "**General**\n",
        "*   When padding, it helps to set a variable, such as \"max_review_length\", if your padded sequences must all be the same length.\n",
        "*   Methods that pad data data usually take care of truncating data as well since their purposes and operations are similar. This is the case with Keras.\n",
        "\n",
        "**Interpretting Code**\n",
        "*   \"max_review_length\" is set to 500. This means that each sequence will only be 500 elements long\n",
        "*   Previous arrays in \"X_train\" and \"X_test\" are reinitialized as arrays padded to be a specified length. These new values are returned by the \"pad_sequences()\" method\n",
        "*   To return the padded the data, \"pad_sequences()\" is taking two parameters:\n",
        "  *   \"X_train\" and \"X_test\" refer to the sequences that need to be padded. In this case, padding will be applied to all of the arrays inside of \"X_train\" and \"X_test\"\n",
        "  *   \"maxlen=max_review_length\" tells the method how each array must be padded to have 500 elements\n",
        "  \n",
        "**External Information**\n",
        "*   [Padding, Truncating, and Rounding](https://www.databasejournal.com/features/mssql/article.php/10894_2222111_3/Padding-Rounding-Truncating-and-Removing-Trailing-Zeroes.htm)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "BHx3ztxU4kyS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Defining the model (Hyper-opt: hyperparameter optimization)\n",
        "embedding_vector_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y1hu_n0f4k8D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Recurrent Neural Network Structure**\n",
        "\n",
        "This cell builds the recurrent neural network which has some unique requirements compared to other neural networks.\n",
        "\n",
        "**Vocabulary/Concepts**\n",
        "*   Model..........................Synonym for \"neural network\"\n",
        "*   Neurons......................The functional unit of any neural network. Conduct specialized operations on input data.\n",
        "*   Layers..........................Assortments of neurons in parallel layers. Each neuron in a layer has one output. All outputs of one layer is fed into each neuron in the next layer. A layer is usually tied to a specific function\n",
        "  *   Embedding.................A layer that must come first if included in a network. The embedding layer assigns numerical values to words that represent meaning\n",
        "  *   LSTM..........................A layer of specialized neurons that determine what the network should remember and forget to avoid the vanishing gradient dilemma\n",
        "  *   Dense.........................A layer of plain neurons that manipulate numerical input\n",
        "  *   Dropout......................Sets a proportion of the inputs going into the next layer as zero. This forces other neurons to learn to prevent overfitting\n",
        "*   Activation Functions...A function within a neuron that manipulates the output of the neuron by determining whether or not the neuron activates\n",
        "  *   Sigmoid......................Ensures the output of a single neuron is between 0 and 1. This is used for binary classification or classification between two categories\n",
        "*   Vanishing Gradient......A dilemma in recurrent neural networks where prolonged neural network operation leads to meaningless outputs close to zero\n",
        "\n",
        "**Execution**\n",
        "\n",
        "The first line initializes the variable \"embedding_vector_length\" to 32. The next line makes building a neural network as easy as layering a cake; any layer added to the model after \"model = Sequential()\" is layered above of the previous layer where the first layer added is the input layer. The first layer converts words into numerical values based on a vector that charts the words' meaning. The two dropout layers randomly make a certain percentage of the outputs from the previous layer equal to zero to force other neurons to learn. The LSTM layer takes the outputs of the previous layer and manipulates them like a classic neuron. LSTMs also determine what the network remembers and forgets. Lastly, the dense layer is made of classic neurons which manipulate data to classify the reviews as either positive or negative.\n",
        "\n",
        "**General**\n",
        "*   When building a network, there is a lot of customization to consider. For instance, why dropout out 20% of the data when you can dropout more or less? Why only have five layers when you can create more or less? Finding the optimal model by manipulating independent variables, like the model structure, is called \"hyperparameter optimization\".\n",
        "*   The \"sigmoid\" activation function will be used for any output layer when the goal of the network is to classify something into two categories. This is called binary classification.\n",
        "*   The number of neurons in the first layer must be equal to the amount of data being put into it. That's why the \"input_length\" parameter of the method is set equal to our \"max_review_length\" that we initialized earlier.\n",
        "*   The number of neurons in the output layer MUST be equal to the number of categories to classify inputs as. However, in this case, only one neuron is needed because outputs below .5 can be classified as negative and outputs above .5 can be classified as positive.\n",
        "*   The vanishing gradient is a dilemma that arises from a recurrent neural network memorizing too many outputs. This is due to a neuron's activation function having a gradient or derivative between 1 and 0. Thus, when outputs are repeatedly put through the neuron, the future outputs asymptotically approach a limit of zero and become meaningless. LSTMs solve this dilemma by discriminately remembering and forgetting previous outputs.\n",
        "*   When the \"embedding\" layer charts words on a vector based on their \"meaning\", the neural network is evaluating the words given the context they appear in. Thus, the neural network doesn't understand english, but it is capable of guessing a word's relationship to other words. For instance, a network may have nouns clustered together since those types of words have similar use cases in sentences.\n",
        "\n",
        "**Interpretting Code**\n",
        "*   \"max_review_length\" is set to 500. This means that each sequence will only be 500 elements long\n",
        "*   \"model = Sequential()\" affirms that the layers of the neural network will be added in order from the input layer to the output layer. All of these methods are taken from Keras\n",
        "*   \"model.add()\" layers a specified layer on top of the previous layer. This method only takes one parameter of what type of layer should be added, but each layer is created with a method that requires their own unique parameters:\n",
        "*   \"Embedding\" creates a layer that converts words into numerical values that represent the words meaning which is interpretted through context\n",
        "*   \"Dropout\" creates a layer that sets a random proportion of the data going into the next layer to zero which forces other neurons to learn patterns. This combats overfitting and it's takine one parameter:\n",
        "  *   \"0.2\" refers to how much of the input data going into the next layer should be set to zero. The elements are chosen randomly, but half of the data will always be zero\n",
        "*   \"Dense\" creates a layer of normal neurons that don't have a specialized operation beyond data manipulation. It's taking two parameters:\n",
        "  *   \"1\" refers to the number of neurons in the layer. Each neuron will be receiving the outputs of each neuron in the previous layer\n",
        "  *   \"activation=\"sigmoid\"\" sets the activation function to the sigmoid function\n",
        "\n",
        "**External Information**\n",
        "*   [Neurons, Layers, and General Structure](neuralnetworksanddeeplearning.com/chap1.html)\n",
        "*   [LSTMs and Reccurent Neural Network Structure](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "*   [Activation Functions](https://www.quora.com/What-is-the-role-of-the-activation-function-in-a-neural-network-How-does-this-function-in-a-human-neural-network-system)\n",
        "*   [Word Embedding](https://en.wikipedia.org/wiki/Word_embedding)\n",
        "*   [Sigmoid and Softmax](http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/)\n",
        "*   [Vanishing Gradient](https://medium.com/@anishsingh20/the-vanishing-gradient-problem-48ae7f501257)\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4YFSM3n04lFj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Compiling the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MreI2oId4lPQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Compiling**\n",
        "\n",
        "This cell compiles the neural network by providing a menu of common hyperparameter configurations for the user to choose from. Like the structure, these will also vary from network to network depending on the task at hand.\n",
        "\n",
        "**Vocabulary/Concepts**\n",
        "*   Hyperparameters................The aspects of a neural network that a person can edit; independent variables\n",
        "*   Weights................................Weights are contained in neurons and come in the form of regular weights and biases that respectively multiply and add to input values. These are changed in backpropagation according to loss\n",
        "*   Loss Function......................Function that determines how loss is quantified\n",
        "*   Binary Crossentropy............A common loss function for classification problems with two classes. Loss is calculated logarithmically where a greater difference between the network's classification and the actual classification means a greater loss\n",
        "*   Optimizer.............................A custom set of optimized hyperparameters. \"Adam\" includes:\n",
        "  *   Learning Rate (Alpha).....................Determines how dramatically neuron weights are changed in response to loss\n",
        "  *   Gradient Decay (Beta 1)..................Determines how much the gradients lessen the more the network learns\n",
        "  *   Squared Gradient Decay (Beta 2)...Determines how much the gradients' squares lessen the more the network learns\n",
        "  *   Learning Rate Decay.......................Determines how much the learning rate lessens the more the network learns\n",
        "  *   Fuzz Factor (Epsilon)......................A small float that prevents division errors from dividing by zero\n",
        "  *   Amsgrad..........................................A boolean (true or false value) that determines what variant of the \"Adam\" optimizer to use\n",
        "*   Metrics.................................A measurement to look at in order to judge the performance of the model\n",
        "\n",
        "**Execution**\n",
        "\n",
        "Compiling the model covers general hyperparameters that are necessary for every neural network, but again, can vary from network to network. Each hyperparameter has a different purpose. Most hyperparameters can greatly affect the viability of a neural network.\n",
        "\n",
        "**General**\n",
        "*   There are many optimizers, loss functions, metrics, and other hyperparameters to consider when building a neural network. Neural networks themselves are known for specializing in a task in a non-explicit manner, but these hyperparameters can enable a network to reach certain levels of performance. Hyperparameter optimization ensures that hyperparameters are adjusted to reach maximum levels of performance.\n",
        "*   \"Adam\" is a great general-use optimizer for natural language processing.\n",
        "\n",
        "**Interpretting Code**\n",
        "*   \"model.compile()\" configures and initializes the rest of the network's hyperparameters.\n",
        "*   \"loss='binary_crossentropy'\" sets the loss function to binary crossentropy\n",
        "*   \"optimizer='adam'\" sets the optimizer to adam, an optimizer that adjusts the learning rate, gradients' decay, and gradients' squared decay as the network learns\n",
        "*   \"metrics=['accuracy']\" ensures that the network's accuracy is kept track of to measurement its performance\n",
        "\n",
        "**External Information**\n",
        "*   [Hyperparameter Optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization)\n",
        "*   [Loss Functions](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)\n",
        "*   [Keras Optimizers](https://keras.io/optimizers/)\n",
        "*   [Keras Metrics](https://keras.io/metrics/)\n",
        "*   [Adam Optimizer](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
        "*   [Other Optimizers](https://www.quora.com/What-are-differences-between-update-rules-like-AdaDelta-RMSProp-AdaGrad-and-AdaM)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "5F518hGo4lYZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Setting Early Stopping\n",
        "my_callbacks = [EarlyStopping(monitor=\"acc\", patience=5, mode=max)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TU13GidO4lhR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Callbacks**\n",
        "\n",
        "This cell is in charge of callbacks which run certain functions alongside training. This enables the user to get a view on metrics during runtime, track additional statistics, and stop training early. This callback only focuses on stopping training early which is typically done when a neural network is seeing no improvement.\n",
        "\n",
        "**Vocabulary/Concepts**\n",
        "*   Callback............A set of functions that can be executed at any step during training\n",
        "*   EarlyStopping...A callback that tracks the neural network's performance and stops training if it stops improving\n",
        "\n",
        "**Execution**\n",
        "\n",
        "The \"my_callbacks\" object is initialized to be used later during training. By default, Keras's training method, \".fit()\", includes a callback that returns metrics after each epoch/batch the neural network completes. This will help us track the network's improvement during training to determine if it will produce desired results, if it has overfitted, etc.\n",
        "\n",
        "**General**\n",
        "*   Keras training always keeps track of runtime metrics.\n",
        "*   There are various callbacks with different functions like saving a networks's progress, stopping in response to a certain stimuli, streaming the network's progress, and more. \n",
        "\n",
        "**Interpretting Code**\n",
        "*   \"my_callbacks\" is being initialized as a callback object\n",
        "*   \"EarlyStopping()\" constructs the callback object by taking three parameters:\n",
        "  *   \"monitor=\"acc\"\" lets the function know it will monitor accuracy to measure improvement\n",
        "  *   \"patience=5\" tells the function to wait five epochs no improvement before it stops training\n",
        "  *   \"mode=max\" tells the function to stop training when the monitored value stops increasing as opposed to \"mode=min\" which tells the function to stop training when the monitored value stops decreasing\n",
        "  \n",
        "**External Information**\n",
        "*   [Keras Callbacks](https://keras.io/callbacks/)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "u18OwYla4lpB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Fitting & Evaluating\n",
        "hist = model.fit(X_train, y_train, \n",
        "                    epochs=epochs, \n",
        "                    batch_size=batch_size, \n",
        "                    verbose=1, \n",
        "                    validation_split=0.3, \n",
        "                    callbacks=my_callbacks)\n",
        "\n",
        "score = model.evaluate(X_test, y_test)\n",
        "\n",
        "print(\"Testing Loss:\", score[0])\n",
        "print(\"Testing Accuracy:\", score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "86uzvr1y4lxx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Training & Testing**\n",
        "\n",
        "By now, the neural network can finally be trained and tested. This cell trains the neural network, saves its training history, uses the trained network to evaluate a new set of test data, saves its test score, then prints out the scores for the user to see.\n",
        "\n",
        "**Vocabulary/Concepts**\n",
        "*   Fitting..................Synonym for \"training\"; Trial runs on test data to fuel backpropagation which mathematically analyzes error to reduce future error. Backpropagation occurs after a batch is run through and is the source of a network's \"learning\" capabilities\n",
        "*   Validation Data...Miniature test data that tests the network at the end of each epoch. Validation data comes from splitting training data again. In scholastic terms, training data is homework, validation data is a quiz, and test data is a major test\n",
        "*   Evaluating...........Synonym for \"testing\"; Occurs after training to measure how well the machine has learned. A countermeasure to overfitting\n",
        "\n",
        "**Execution**\n",
        "\n",
        "The first line trains the network and then saves its history in an object called \"hist\". While the network trains, it will return metrics on its performance. In the next line of code, the fully trained model is tested with the test data that was set aside earlier to measure the validity of the network's performance. The outputs of the test is saved to \"score\" which is then printed out for the user to read in the next two lines. This cell, and the next couple of cells, give the user data to judge the network.\n",
        "\n",
        "**General**\n",
        "*   Evaluating is suppose to test your trained network, but for comparison, you can evaluate your network before you train it to compare the untrained network to the trained network. Just be sure that it isn't working on real-world data that carry consequences for mistakes.\n",
        "*   It's always wise to save the network's history and test score to view later, especially for hyperparameter optimization.\n",
        "\n",
        "**Interpretting Code**\n",
        "*   \"hist\" is a object that's initialized as the history returned by the \".fit()\" method\n",
        "*   \"model.fit()\" is a method from Keras that trains the neural network. It's taking seven parameters:\n",
        "  *   \"x_train\" refers to the training movie reviews that will be input into the network\n",
        "  *   \"y_train\" refers to the training labels that will be compared to the network's output\n",
        "  *   \"batch_size=batch_size\" sets the batch size for Keras's training equal to our batch size that we initialized earlier\n",
        "  *   \"epochs=epochs\" sets the epochs for Keras's training equal to our epochs that we also initialized earlier\n",
        "  *   \"verbose=1\" tells Keras how verbose it should be regarding the network's training progress. \"0\" tells Keras to stay silent. \"1\" tells Keras to show a progress bar. \"2\" tells Keras to show a bar per epoch\n",
        "  *   \"validation_split=0.3\" tells Keras to split training data once again into training data and validation data (with corresponding labels). It's exactly like the test_train_split() from sklearn, except this happens inside training\n",
        "  *   \"callbacks=my_callbacks\" sets the callbacks for Keras's training equal to the callbacks we initialized earlier\n",
        "*   \"score\" is a tuple that's initialized to hold the accuracy and loss of the model which is returned by the \".evaluate()\" method\n",
        "*   \"model.evaluate()\" is a method from Keras that tests the neural network. It's taking two parameters:\n",
        "  *   \"x_test\" refers to the test movie reviews that were set aside earlier. These will be fed into our trained neural network to judge its performance\n",
        "  *   \"y_test\" refers to the test labels that were set aside earlier. These will also be fed into our trained neural network to judge its performance\n",
        "*   \"print(\"Testing Loss:\", score[0])\" prints the loss of the model from the testing session which is stored as the first element of the \"score\" tuple\n",
        "*   \"print(\"Testing Accuracy:\", score[1])\" prints the accuracy of the model from the testing session which is stored as the second element of the \"score\" tuple\n",
        "\n",
        "**External Information**\n",
        "*   [Testing, Training, and Validation Data](https://stackoverflow.com/questions/2976452/whats-is-the-difference-between-train-validation-and-test-set-in-neural-netwo)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "aM-R_TGh4l5Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Model Summary\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TumTCy8P4mBT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Neural Network Summary**\n",
        "\n",
        "This cell gives a summary of the model that tells the user the shape of the data as it moves through the neural network and the qualities of each layer's neuron parameters which are the neuron's weights and biases.\n",
        "\n",
        "**Vocabulary/Concepts**\n",
        "*   Neuron Parameters...Refers the to weights and biases in a neuron which can be altered by backpropagation but some neuron parameters can be trained and others not\n",
        "\n",
        "**Execution**\n",
        "\n",
        "This line of code simple describes the model the user has built in more depth using ther \".summary()\" method from Keras. This is useful for keeping track of what layers are being used, the output shape of each layer, and the number of neuron parameters each layer has that can be edited.\n",
        "\n",
        "**General**\n",
        "*   Outputs from layer to layer are expected to decrease since classification problems typically classify data into a small number of categories based on a large amount of input data.\n",
        "*   Not every layer has revisable parameters. Instead, they carry out an operation on the outputs of the previous layer.\n",
        "*   Dense layers will typically have the most neuron parameters and account for most of the data manipulation.\n",
        "\n",
        "**Interpretting Code**\n",
        "*   \"model.summary()\" prints out a description of the neural network model that the user has created which quickly calculates the number of neuron parameters in each layer and displays the output shape of each layer. The \".summary()\" method is a useful tool for quick information about a network\n",
        "\n",
        "**External Sources**\n",
        "*   [Neuron Structure](https://www.neuraldesigner.com/blog/perceptron-the-main-component-of-neural-networks)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Em07g8Q24mI8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Plotting training accuracy & validation accuracy\n",
        "epoch_list = list(range(1, len(hist.history['acc']) + 1))\n",
        "plt.plot(epoch_list, hist.history['acc'], epoch_list, hist.history['val_acc'])\n",
        "plt.legend((\"Training Accuracy\", \"Validation Accuracy\"))\n",
        "plt.show"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TE7yc1q94mRo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Visualizing Metrics**\n",
        "\n",
        "This cell creates a graph to visualize the development of our network over each epoch with the help of a library called Matplotlib.\n",
        "\n",
        "**Vocabulary/Concepts**\n",
        "*   Convergence...............When the changes of the metrics in a neural network (like loss or accuracy) approach zero, the network is \"converging\". A network reaches convergence when the metrics start to plateau over epochs; they begin to reach a limit\n",
        "*   Training Accuracy.......How accurate the network is at classifying training data\n",
        "*   Validation Accuracy....How accurate the network is at classifying validation data\n",
        "\n",
        "**Execution**\n",
        "\n",
        "The first line of code initializes \"epoch_list\" as a list of numbers starting from 1 to the total number of epochs plus one. Second, data is stored on a chart using Matplotlib where the training accuracy and validation accuracy are plotted with respect to the number of epochs. Then, a legend is created on the chart which denotes the dependent variables: \"Training Accuracy\" and \"Validation Accuracy\". Lastly, the plot is shown. This tool, like the summary and callbacks, are useful in comparing neural networks for hyperparameter optimization or judging a network by visualizing metrics.\n",
        "\n",
        "**General**\n",
        "*   When plotting, it is nice to include a legend and title so readers can understand what data is being plotted.\n",
        "*   \"plt.show\" is necessary to display a graphic of the chart. From there, the chart can be saved as an image, exported, printed, etc.\n",
        "\n",
        "**Interpretting Code**\n",
        "*   \"epoch_list\" is a list that will store epoch numbers as individual elements\n",
        "*   \"list()\" creates a list of elements. The elements are specified by \"range(1, len(hist.history['acc']) + 1))\". Essentially, every integer inclusively between 1 and the length of the \"hist.history['acc']\" array plus one (which should be 50) becomes an element in the list\n",
        "*   \"plt.plot()\" is a Matplotlib method that stores data in a plot. It's taking four parameters that come in pairs:\n",
        "  *   \"epoch_list\" is the list that was initialized earlier. This series of integers will serve as the x axis of the plot for both \"hist.history['acc']\" and \"hist.history['val_acc']\" which is why \"epoch_list\" appears twice\n",
        "  *   \"hist.history['acc']\" is an array that stores the accuracy of the neural network from epoch to epoch. This is a dependent variable that will be plotted on the y axis\n",
        "  *   \"hist.history['val_acc']\" is an array that stores the validation accuracy of the neural network from epoch to epoch. This is also a dependent variable that will be plotted on the y axis\n",
        "*   \"plt.legend()\" is a Matplotlib method that creates a legend on the chart. It's taking one parameter:\n",
        "  *   \"(\"Training Accuracy\", \"Validation Accuracy\")\" is a tuple whose two elements refer to the previously plotted pairs. The first element of the tuple tells Matplotlib that the first pair of independent and dependent variables are measuring training accuracy. The second element tells Matplotlib that the second pair of variables measure validation accuracy.\n",
        "*   \"plt.show\" is Matplotlib method that is necessary for showing the chart the user created beforehand. From there, the chart can be saved as an image, printed, exported, etc.\n",
        "\n",
        "**External Information**\n",
        "*   [Convergent Series](https://en.wikipedia.org/wiki/Convergent_series)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "60yAak-1BHIL",
        "colab_type": "code",
        "outputId": "35cfaae7-cdad-473b-ae14-b18d14d3938f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1047
        }
      },
      "cell_type": "code",
      "source": [
        "#Entire Program\n",
        "\n",
        "#Import necessary libraries\n",
        "import keras\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "\n",
        "#Suppress Warnings (Optional)\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\"\n",
        "\n",
        "#Set Variables\n",
        "epochs = 3\n",
        "batch_size = 64\n",
        "\n",
        "#Load and separate data into X: Features, Y: Labels\n",
        "top_words = 5000 #Loads top 5000 words\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "print(X_train)\n",
        "\n",
        "# truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "\n",
        "#Defining the model (Hyper-opt: hyperparameter optimization)\n",
        "embedding_vector_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#Compiling the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#Setting Early Stopping\n",
        "my_callbacks = [EarlyStopping(monitor=\"acc\", patience=5, mode=max)]\n",
        "\n",
        "#Fitting & Evaluating\n",
        "hist = model.fit(X_train, y_train, \n",
        "                    epochs=epochs, \n",
        "                    batch_size=batch_size, \n",
        "                    verbose=1, \n",
        "                    validation_split=0.3, \n",
        "                    callbacks=my_callbacks)\n",
        "\n",
        "score = model.evaluate(X_test, y_test)\n",
        "\n",
        "print(\"Testing Loss:\", score[0])\n",
        "print(\"Testing Accuracy:\", score[1])\n",
        "\n",
        "#Model Summary\n",
        "model.summary()\n",
        "\n",
        "#Plotting training accuracy & validation accuracy\n",
        "epoch_list = list(range(1, len(hist.history['acc']) + 1))\n",
        "plt.plot(epoch_list, hist.history['acc'], epoch_list, hist.history['val_acc'])\n",
        "plt.legend((\"Training Accuracy\", \"Validation Accuracy\"))\n",
        "plt.show"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "[list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32])\n",
            " list([1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 4369, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])\n",
            " list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 2, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])\n",
            " ...\n",
            " list([1, 11, 6, 230, 245, 2, 9, 6, 1225, 446, 2, 45, 2174, 84, 2, 4007, 21, 4, 912, 84, 2, 325, 725, 134, 2, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 2, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 2, 1209, 2295, 2, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 2, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 2, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 2, 5, 27, 710, 117, 2, 2, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 2, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 2, 2, 5, 4241, 18, 4, 2, 2, 250, 11, 1818, 2, 4, 4217, 2, 747, 1115, 372, 1890, 1006, 541, 2, 7, 4, 59, 2, 4, 3586, 2])\n",
            " list([1, 1446, 2, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 2, 112, 2, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 2, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23])\n",
            " list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 2, 9, 24, 6, 78, 1099, 17, 2345, 2, 21, 27, 2, 2, 5, 2, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 2, 2, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 2, 8, 2197, 2, 2, 544, 5, 383, 1271, 848, 1468, 2, 497, 2, 8, 1597, 2, 2, 21, 60, 27, 239, 9, 43, 2, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:511: RuntimeWarning: EarlyStopping mode <built-in function max> is unknown, fallback to auto mode.\n",
            "  RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 17500 samples, validate on 7500 samples\n",
            "Epoch 1/3\n",
            "17500/17500 [==============================] - 188s 11ms/step - loss: 0.5158 - acc: 0.7338 - val_loss: 0.3758 - val_acc: 0.8367\n",
            "Epoch 2/3\n",
            "17500/17500 [==============================] - 189s 11ms/step - loss: 0.3235 - acc: 0.8686 - val_loss: 0.3630 - val_acc: 0.8477\n",
            "Epoch 3/3\n",
            "17500/17500 [==============================] - 194s 11ms/step - loss: 0.2933 - acc: 0.8833 - val_loss: 0.3520 - val_acc: 0.8575\n",
            "25000/25000 [==============================] - 71s 3ms/step\n",
            "Testing Loss: 0.34870034334182737\n",
            "Testing Accuracy: 0.85916\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 500, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFKCAYAAAAnj5dkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f411503cc50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}