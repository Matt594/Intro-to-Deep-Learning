{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST Multi Layer Perceptrons",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matt594/Intro-to-Deep-Learning/blob/master/MNIST_Multi_Layer_Perceptrons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ZyhFqmCLhz-s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Interpretting Multi Layer Perceptrons**\n",
        "\n",
        "This notebook is a breakdown of multi layer perceptrons with Keras. The goal of this notebook is to provide a compact reference for those with an entirely fresh background to computer science and/or machine learning. This is also just one out of three notebooks; the other two notebooks tackle convolutional neural networks and recurrent neural networks.\n",
        "\n",
        "A multi layer perceptron is a deep learning structure commonly used to classify data where there is no overarching pattern between different elements that contribute to the data's interpretation. For instance, interpretting a list of people's height is different from interpretting a sentence; reading the heights out of sequence makes sense while reading a sentence out of sequence doesn't. Because of the multi layer perceptron's straightforward interpretation, it doesn't have the complexities other networks possess. This makes the multi layer perceptron great for introducing deep learning. In this notebook, our multi layer perceptron will classify small images of handwritten digits into ten categories. Of course, larger images will need to be classified by convolutional neural networks due to the greater influence of spatial patterns, but that is beyond the scope of this notebook.\n",
        "\n",
        "In each cell, I will attempt to provide a grounds-up explanation of the cell's contents, so the cell explanations may stray from machine learning concepts time to time and repeat content provided by the two other notebooks. Again, the goal of this notebook is to provide a reference for anyone to understand while remaining compact. If there's a topic you'd like to look into more, I've provided some extra sources of information in each cell.\n",
        "\n",
        "Description of code cells should include:\n",
        "1. Title: What topic(s) are being covered in the cell\n",
        "2. Purpose: Goal of the cell\n",
        "3. Execution: What the code is doing\n",
        "4. Vocabulary/Concepts: Terminology and concepts for computer science of machine learning\n",
        "5. General: Common principle/practice regarding cell operations and code\n",
        "6. Interpretating Code: A guide to deciphering the cell's code\n",
        "7. External Information: Sources for more information"
      ]
    },
    {
      "metadata": {
        "id": "e_2pIUTfwQFr",
        "colab_type": "code",
        "outputId": "6f787f4b-981c-43ec-9108-bebc992618c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Import necessary libraries\n",
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "2rYY0ZhVh1II",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Import Statements**\n",
        "\n",
        "This first cell imports modules to make the coding process simpler. With pre-made modules, the programmer doesn't have to reinvent the wheel. They can spend more time creating an applicable program and less time creating a modular foundation.\n",
        "\n",
        "**Vocabulary/Concepts**\n",
        "*   Importing...Makes data accessible from another module\n",
        "  *   Module.......Collection of pre-made methods, classes, or programs\n",
        "\n",
        "**Execution**\n",
        "\n",
        "When a program calls for an import, the program searches for a module of the requested name on a system path.Though, before importing, modules must be downloaded or created so they have an address on the machine where the program is running. Once again, these are crucial because they'll save us time when creating our network; we won't need to create one from scratch.\n",
        "\n",
        "**General** \n",
        "*   When importing, distinguish what modules must be used. No program needs to know every module on your machine.\n",
        "*   Chunk imports together that are related to each other.\n",
        "*   When importing created modules, like classes, it's best to keep all of the program's modules in one file so you don't need to change the system path.\n",
        "*   Not every module is supported over multiple programming languages.\n",
        "*   If you're having trouble with a particular task, look up modules for your programming languageâ€”you may find something incredibly useful to import.\n",
        "\n",
        "**Interpretting Code**\n",
        "*   \"import\" calls for modules to import\n",
        "*   \"from\" specifies what should be imported from a module rather than importing the entire module\n",
        "*   \"as\" changes how the module is called when it's used in the program\n",
        "*   \"keras\" is a machine learning module for python that simplifies Google's Tensorflow\n",
        "*   \"matplotlib\" is a data plotting module\n",
        "*   \"sklearn\" is data mining and data analysis module\n",
        "*   \"os\" is a python module for your machine's operating system\n",
        "*   \"future\" is a python module that integrates code from a future python version into a past version\n",
        "\n",
        "**External Information**\n",
        "*   [Future](https://stackoverflow.com/questions/7075082/what-is-future-in-python-used-for-and-how-when-to-use-it-and-how-it-works)\n",
        "*   [Python Import System](https://docs.python.org/3/reference/import.html)\n",
        "*   [Keras Documentation](https://keras.io)\n",
        "*   [Matplotlib Documentation](https://matplotlib.org)\n",
        "*   [Python Documentation](https://www.python.org)\n",
        "*   [Future](https://stackoverflow.com/questions/7075082/what-is-future-in-python-used-for-and-how-when-to-use-it-and-how-it-works)\n",
        "\n",
        "These sources are great for interpretting what modules are being used in this notebook. If there is any uncertainty as to what operation is being done, it never hurts to look up the operation at the documentation's source.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "VGtbUQrrh1Yv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Suppress Warnings (Optional)\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9d0AGJf9h1nZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Variable Initialization & Environmental Variables**\n",
        "\n",
        "This cell suppresses warnings and disables debugging logs from Tensorflow. This is mostly just for preference since the program can operate without this line of code, but it makes the program notifications less annoying.\n",
        "\n",
        "**Vocabulary/Concepts**\n",
        "*   Variable Initialization.......Assigns a value to a variable\n",
        "  *   Variable............................Stores data under a name and type\n",
        "  *   Data..................................Information\n",
        "  *   Data type..........................Form data takes to represent itself which includes integers, floats, strings, etc.\n",
        "*   Environmental Variable...A value that affects running process behavior\n",
        "\n",
        "**Execution**\n",
        "\n",
        "Variable initilization sets one variable equal to a value. In this case, an environmental variable is being set to the value \"2\" which will be relayed to Tensorflow in order to prevent debugging and warnings from being displayed. Again, this statement was included as a preference since the notifications can be distracting, but the network can operate without suppressing Tensorflow's warnings.\n",
        "\n",
        "**General**\n",
        "*   Environmental variables can alter the way your program runs so it is important to be aware of how each variable affects the program when coding. \n",
        "*   Variable initialization is a basic computer science operation. It is crucial in almost every program. \n",
        "\n",
        "**Interpretting Code**\n",
        "*   \"os\" refers to the python operating system module\n",
        "*   \"environ['TF_CPP_MIN_LOG_LEVEL']\" specifies the environmental variable being changed\n",
        "*   \"=\" is an operator where the variable on the left side is set equal to the value on the right\n",
        "*   \"2\" is the value that the environmental variable is set to which is one of the modes for tensorflow logging\n",
        "\n",
        "**External Information**\n",
        "*   [Python Operators](https://www.tutorialspoint.com/python/python_basic_operators.htm)\n",
        "*   [Variable Intialization](https://stackoverflow.com/questions/23345554/the-differences-between-initialize-define-declare-a-variable)\n",
        "*   [Environmental Variables](https://en.wikipedia.org/wiki/Environment_variable)\n",
        "*   [Operating System Documentation](https://docs.python.org/2/library/os.html)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "0xCKwGsdh1yT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Set Variables\n",
        "epochs = 20\n",
        "num_classes = 10\n",
        "batch_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nix36G0ph166",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **More Variable Initialization & ML Variables**\n",
        "\n",
        "This cell initializes more variables that will be used for training and classification.\n",
        "\n",
        "**Vocabulary/Concepts**\n",
        "*   Epochs.................................Times a network runs through an entire dataset\n",
        "*   Classes (Classification)......Number of categories a network can classify data into\n",
        "*   Batches................................A group of training examples. The network can only update after training through all the examples in the batch. Important information from each example is saved for updating until the batch is completed\n",
        "  *   Update...............................Synonym for backpropagate. Backpropagation occurs after each batch during training and will be discussed in the next cell\n",
        "\n",
        "**Execution**\n",
        "\n",
        "Each line of code is initializing a separate variable which all take the form of integers. These variables will be used later to instruct the neural network how long it should run for, what categories to classify data into, how many examples it should update after, and how many inputs it will need to digest.\n",
        "\n",
        "**General**\n",
        "*   You may wonder \"why initialize a variable if it's just a number that can be hard-coded?\" This highlights two more important aspect of variable initialization: labeling and multiple usage.\n",
        "  *   Labeling numbers gives others a better understanding of what is being used. For example, a method that uses \"epochs\" as an input tells the reader more than a method that uses \"50\" as an input.\n",
        "  *   Using a value multiple times without assigning it to a variable can lead future problems. For example, if you use \"50\" everywhere and you want to change all 50s to 40s, you'll have manually change every occurence of 50. If you assign 50 to \"epochs\", you can change the variable's value without manually finding every instance of the variable.\n",
        "*   Group your initialized variables together; they'll be easier to find if you need to change their values.\n",
        "\n",
        "**Interpretting Code**\n",
        "*   \"epochs\" is set to 20. This means the network will run through the entire training dataset 50 times\n",
        "*   \"num_classes\" is set to 10. This means the network must classify an image into one of 10 classes\n",
        "*   \"batch_size\" is set to 128. This means the network will run through 128 images before updating\n",
        "\n",
        "**External Information**\n",
        "*   [Epochs, Batches, & Iterations](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "5lM9fWHfh2Di",
        "colab_type": "code",
        "outputId": "03335369-d54d-4bdf-b9a2-6f33ee9dc3d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#Load and separate data into X: Features, Y: Labels\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7_K0dAgQh2MI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Training Data & Testing Data, Plus Methods**\n",
        "\n",
        "This cell initializes the mnist data into training data with training labels and test data with test labels which will later be interpretted by the neural network.\n",
        "\n",
        "**Vocabulary/Concepts**\n",
        "*   Methods......................Operations under a name that accomplish a task\n",
        "*   Data.............................Information. In this case, the data are words\n",
        "*   Labels..........................The identification of data\n",
        "*   Tuples, Arrays, Lists...Series of data\n",
        "*   Training data...........Data the algorithm evaluates to backpropagate and improve\n",
        "*   Test data.................Data the algorithm evaluates so people can measure the algorithms's performance\n",
        "*   Training.....................Trial runs on training data to fuel backpropagation\n",
        "  *   Backpropagation......Mathematical analysis of loss to reduce future loss. Backpropagation occurs after a batch is completed. It is the source of a network's \"learning\" capabilities\n",
        "  *   Loss...........................Also known as error. Measures how far off the network's classification is from the true value\n",
        "*   Testing......................Occurs after training to measure how well the machine has learned. Testing is a countermeasure to overfitting\n",
        "  *   Overfitting.................Dilemma that occurs when an algorithm gets extremely good at classifying training data, but performs worse when classifying new data. Essentially, the machine has memorized the training data and ignores the patterns that can apply to new data\n",
        "  \n",
        "**Execution**\n",
        "\n",
        "The \"load_data()\" method from keras returns a tuple of tuples. These tuples will be initialized as a training data and training label tuple and a test data and test label tuple respectively. This split provides data for both training and testing of the neural network. However, the data is still not ready to be used.\n",
        "\n",
        "**General**\n",
        "*   Methods and objects can be created by the programmer or can come from imported modules. They can be applied to class objects or can be applied using the import calls. Methods also often require parameters (which are placed between the parentheses) and can return a value. In this case, the \".load_data()\" method returns a tuple of tuples.\n",
        "*   There are three types of data in series:\n",
        "  *   Tuples are denoted by parentheses, are limited in size, can carry different objects, and cannot be edited once created. \n",
        "  *   Lists are denoted by brackets, are unlimited in size, can carry different objects, and can be edited once created. \n",
        "  *   Arrays are denoted by brackets and are either created with the Python array module or the Numpy module. They are unlimited in size, carry one type of object, and can be edited once created. Additionally, they're valuable since they can undergo manipulation that affects the entire array.\n",
        "*   Keep in mind that the elements in the tuples inside the tuple refer to a two-dimensional array of elements that represent images.\n",
        "*   It is important to note that arrays, lists, and tuples follow a specific rule when they are indexed. For instance, let's assume \"arr\" is an array of 10 integers in sequence from 1 to 10.  arr[0] refers to the first integer in the array: 1. arr[1] refers to the second integer: 2. At the very end, arr[9] refers to 10. arr[10] would throw an error because it is out of bounds. Thus, when indexing or creating one of these three objects, always keep in mind that the index starts counting at [0] and ends at [(range) - 1].\n",
        "*   Training data should be around twice as large as test data.\n",
        "*   Don't let your machine train for too many epochs otherwise you risk overfitting\n",
        "*   To gauge when your algorithm begins to overfit, keep track of metrics by printing out information as the machine trains\n",
        "*  \"x\" in machine learning always refers to the input data.\n",
        "*  \"y\" in machine leanring always refers to the labels.\n",
        "\n",
        "**Interpretting Code**\n",
        "*   \"load_data()\" is being applied to the mnist database to return the tuple of tuples of images.\n",
        "*   \"(x_train, y_train), (x_test, y_test)\" are tuples set equal to the tuples returned by the \"load_data()\" method\n",
        "\n",
        "**External Information**\n",
        "*   [Lists, Arrays, and Tuples](https://stackoverflow.com/questions/626759/whats-the-difference-between-lists-and-tuples)\n",
        "*   [Array Indexing](https://en.wikipedia.org/wiki/Array_data_structure#Element_identifier_and_addressing_formulas)\n",
        "*   [Methods](https://en.wikipedia.org/wiki/Method_(computer_programming)\n",
        "*   [Training and Test Data](https://www.quora.com/What-is-a-training-data-set-test-data-set-in-machine-learning-What-are-the-rules-for-selecting-them)\n",
        "*   [Backpropagation](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)\n",
        "*   [Overfitting](https://elitedatascience.com/overfitting-in-machine-learning)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "bR7MmbuNh2Th",
        "colab_type": "code",
        "outputId": "05648b6d-8afc-4101-efec-42b25120a5e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#Data formatting\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "#Notes\n",
        "#*Not converting array types into floats increases loss\n",
        "#*Division seems to have no effect\n",
        "#*Are astype, reshape, and shape from numpy? There is no import for numpy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-d_nj4pfh2a5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Data Formatting**\n",
        "\n",
        "This cell does more data formatting operations on the image arrays by converting their stored variable types and doing some arraywide division.\n",
        "\n",
        "**Vocabulary/Concepts**\n",
        "*   Data type...Form data takes to represent itself\n",
        "*   Float..........A number with a decimal\n",
        "*   Integer.......A whole number\n",
        "\n",
        "**Execution**\n",
        "\n",
        "The first two lines reinitialize \"x_train\" and \"x_test\" into new shapes. The second two lines of code reinitialize \"x_train\" and \"x_test\" once again, except now all of their elements are floats rather than integers. The next two lines of code divide every element in both arrays by 225 and 224 respectively. Lastly, the training data and test data are displayed. These operations ensure that the neural network runs smoothly and takes more precise measurements when analyzing data. \n",
        "\n",
        "**General**\n",
        "*   Floats are generally more precise than integers.\n",
        "*   There are many data types, but numerical data is the least complicated manipulate with machine learning.\n",
        "\n",
        "**Interpretting Code**\n",
        "*   \".reshape()\" is a numpy method that changes the dimensions of an array which is taking two parameters:\n",
        "  *   \"60000\" and \"10000\" refer to the number of rows in the reshaped array\n",
        "  *   \"784\" refers to the number of columns in the reshaped array\n",
        "*   \".astype()\" is a numpy method that casts elements of an array into a new data type which is taking one parameter:\n",
        "  *    \"'float32'\" denotes what datatype the data should be casted into\n",
        "*   \"/=\" is an operator that divides the variable on the left by the value on the right. Then, it sets the variable equal to the quotient\n",
        "*   The first two lines of code reinitialize \"x_train\" and \"x_test\" into arrays of floats\n",
        "*   The last two lines of code divide \"x_train\" and \"x_test\" by 225 and 224 respectively\n",
        "\n",
        "**External Information**\n",
        "*   [Python Data Types](https://realpython.com/python-data-types/#floating-point-numbers)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "vyOYAzTPh2j5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# convert class vectors to binary class matrices (one-hot encoding)\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cjUy8GLnh2ti",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **One-Hot Encoding**\n",
        "\n",
        "This cell does more data formatting with the labels rather than the image arrays. \n",
        "\n",
        "**Vocabulary/Concepts**\n",
        "*   One-hot encoding...Vector representation for labelling data. For example, [1, 0, 0], [0, 1, 0], [0, 0, 1] are all different identities in a list of three elements\n",
        "\n",
        "**Execution**\n",
        "\n",
        "The \"y_train\" and \"y_test\" array elements are all reinitialized as one-hot encoded labels. This enables for the network's output to be placed into a one-dimensional array which allows for an easy, side-by-side comparison between the one-hot label and one-dimensional output array of the network. The network's job is to make them as identical as possible by reducing loss. After this cell runs, the data is now ready to be used by the network.\n",
        "\n",
        "**General**\n",
        "*   One-hot encoding is typically used to label data because it matches the number of output neurons to the number of classes. This is a representation of data that is good to know when getting into classification problems using machine learning.\n",
        "\n",
        "**Interpretting Code**\n",
        "*  \".to_categorical()\" is a keras.utils method that changes labels into a one-hot representation that is taking two parameters:\n",
        "  *    \"y_train\" and \"y_test\" tell the method what labels need to be encoded \n",
        "  *    \"num_classes\" tess the method how many labels need to created\n",
        "*  Both the training and test labels are being coverted into one-hot representations.\n",
        "\n",
        "**External Information**\n",
        "*   [One-hot encoding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "IB1xJLbQh21D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Defining the model (Hyper-opt: hyperparameter optimization)\n",
        "# Hyperparams: Kernal size, node amounts, max pool size\n",
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1gf88e9ah28i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Multi Layer Perceptron Structure**\n",
        "\n",
        "This cell builds the multi layer perceptron. It doesn't have special elements that other networks possess, but other networks often have a similar skeletal structure.\n",
        "\n",
        "**Vocabulary/Concepts**\n",
        "*   Model..........................Synonym for \"neural network\"\n",
        "*   Neurons......................The functional unit of any neural network. Conduct specialized operations on input data.\n",
        "*   Layers..........................Assortments of neurons in parallel layers. Each neuron in a layer has one output. All outputs of one layer is fed into each neuron in the next layer. A layer is usually tied to a specific function\n",
        "  *   Dense.........................A layer of plain neurons that manipulate numerical input\n",
        "  *   Dropout......................Sets a proportion of the inputs going into the next layer as zero. This forces other neurons to learn to prevent overfitting\n",
        "*   Activation Functions...A function within a neuron that manipulates the output of the neuron by determining whether or not the neuron activates\n",
        "  *   Rectified Linear Unit..A good general-use activation function for simple problems\n",
        "  *   Softmax......................Ensures the output of a single neuron is between 0 and 1. Additionally, all neuron outputs will add up to 1. These are desirable for one-hot representation comparisons\n",
        "\n",
        "**Execution**\n",
        "\n",
        "The first line of code makes building a neural network as easy as layering a cake; any layer added to the model after \"model = Sequential()\" is layered above of the previous layer where the first layer added is the input layer. The dense layers are added to manipulate the data from the last layer. The next line takes that condensed information and manipulates it with a layer of plain neurons. The two dropout layers randomly make a certain percentage of the outputs from the previous layer equal to zero to force other neurons to learn. Last is the output layer where the outputs of the ten neurons will be placed in an array and compared to one-hot labels.\n",
        "\n",
        "**General**\n",
        "*   When building a network, there is a lot of customization to consider. For instance, why use the \"relu\" activation function when there are plenty of others to use? Why only have eight layers when you can create more or less? Finding the optimal model by manipulating independent variables, like the model structure, is called \"hyperparameter optimization\".\n",
        "*   \"relu\" is a good general-use activation function that works well in most circumstances. However, with a more complex or niche problem, it may be replaced with another activation function.\n",
        "*   The \"softmax\" activation function will be used for any output layer when the goal of the network is to classify something using one-hot vector comparisons.\n",
        "*   The number of neurons in the first layer must be equal to the amount of data being put into it. That's why the \"input_shape\" parameter of the method is set equal to our \"input_shape\" that we initialized earlier.\n",
        "*   The number of neurons in the output layer MUST be equal to the number of categories to classify images as. That's why the number of neurons parameter is \"num_classes\" which was also initialized earlier.\n",
        "\n",
        "**Interpretting Code**\n",
        "*   \"model = Sequential()\" affirms that the layers of the neural network will be added in order from the input layer to the output layer. All of these methods are taken from Keras\n",
        "*   \"model.add()\" layers a specified layer on top of the previous layer. This method only takes one parameter of what type of layer should be added, but each layer is created with a method that requires their own unique parameters:\n",
        "*   \"Dense\" creates a layer of normal neurons that don't have a specialized operation beyond data manipulation. It's taking two parameters:\n",
        "  *   \"512\" refers to the number of neurons in the layer. Each neuron will be receiving the outputs of each neuron in the previous layer\n",
        "  *   \"activation=\"relu\"\" sets the activation function to the rectified linear unit. At the end, \"activatio=\"softmax\"\" sets the activation function to the softmax function\n",
        "*   \"Dropout\" creates a layer that sets a random proportion of the data going into the next layer to zero which forces other neurons to learn patterns. This combats overfitting and it's takine one parameter:\n",
        "  *   \"0.2\" refers to how much of the input data going into the next layer should be set to zero. The elements are chosen randomly, but half of the data will always be zero\n",
        "\n",
        "**External Information**\n",
        "*   [Neurons, Layers, and General Structure](neuralnetworksanddeeplearning.com/chap1.html)\n",
        "*   [Activation Functions](https://www.quora.com/What-is-the-role-of-the-activation-function-in-a-neural-network-How-does-this-function-in-a-human-neural-network-system)\n",
        "*   [Keras Model Documentation](https://keras.io/models/model/)\n",
        "*   [Sigmoid and Softmax](http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/)"
      ]
    },
    {
      "metadata": {
        "id": "E_sS91Q0h3Ei",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Compile Model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k-Y60DvSh3Mq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Compiling**\n",
        "\n",
        "This cell compiles the neural network by providing a menu of common hyperparameter configurations for the user to choose from. Like the structure, these will also vary from network to network depending on the task at hand.\n",
        "\n",
        "**Vocabulary/Concepts**\n",
        "*   Hyperparameters.......The aspects of a neural network that a person can edit; independent variables\n",
        "*   Weights................................Weights are contained in neurons and come in the form of regular weights and biases that respectively multiply and add to input values. These are changed in backpropagation according to loss\n",
        "*   Loss Function......................Function that determines how loss is quantified\n",
        "*   Categorical Crossentropy...A common loss function for classification problems with more than two classes. Loss is calculated logarithmically where a greater difference between the network's classification and the actual classification means a greater loss\n",
        "*   Optimizer.............................A custom set of optimized hyperparameters. \"RMSprop\"  includes:\n",
        "  *   Learning Rate (Alpha).................Determines how dramatically neuron weights are changed in response to loss\n",
        "  *   Gradient Decay (Rho)..................Determines how much the gradients lessen the more the network learns\n",
        "  *   Learning Rate Decay....................Determines how much the learning rate lessens the more the network learns\n",
        "  *   Fuzz Factor (Epsilon)...................A small float that prevents division errors from dividing by zero\n",
        "*   Metrics.................................A measurement to look at in order to judge the performance of the model\n",
        "\n",
        "**Execution**\n",
        "\n",
        "Compiling the model covers general hyperparameters that are necessary for every neural network, but again, can vary from network to network. Each hyperparameter has a different purpose. Most hyperparameters can greatly affect the viability of a neural network.\n",
        "\n",
        "**General**\n",
        "*   There are many optimizers, loss functions, metrics, and other hyperparameters to consider when building a neural network. Neural networks themselves are known for specializing in a task in a non-explicit manner, but these hyperparameters can enable a network to reach certain levels of performance. Hyperparameter optimization ensures that hyperparameters are adjusted to reach maximum levels of performance.\n",
        "\n",
        "**Interpretting Code**\n",
        "*   \"model.compile()\" configures and initializes the rest of the network's hyperparameters.\n",
        "*   \"loss=keras.losses.categorical_crossentropy\" sets the loss function to categorical crossentropy\n",
        "*   \"optimizer=keras.optimizer.RMSprop()\" sets the optimizer to RMSprop, an optimizer that adjusts the learning rate and gradient decay according to recent network updates\n",
        "*   \"metrics=['accuracy']\" ensures that the network's accuracy is kept track of to measurement its performance\n",
        "\n",
        "**External Information**\n",
        "*   [Hyperparameter Optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization)\n",
        "*   [Loss Functions](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)\n",
        "*   [Keras Optimizers](https://keras.io/optimizers/)\n",
        "*   [Keras Metrics](https://keras.io/metrics/)\n",
        "*   [Other Optimizers](https://www.quora.com/What-are-differences-between-update-rules-like-AdaDelta-RMSProp-AdaGrad-and-AdaM)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "SogzUc9plddV",
        "colab_type": "code",
        "outputId": "32232da3-1660-498a-d820-9f34f6de22c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "cell_type": "code",
      "source": [
        "#Setting Early Stopping\n",
        "my_callbacks = [EarlyStopping(monitor=\"acc\", patience=5, mode=max)]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:511: RuntimeWarning: EarlyStopping mode <built-in function max> is unknown, fallback to auto mode.\n",
            "  RuntimeWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "A2YRgN1Ildvb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Callbacks**\n",
        "\n",
        "This cell is in charge of callbacks which run certain functions alongside training. This enables the user to get a view on metrics during runtime, track additional statistics, and stop training early. This callback only focuses on stopping training early which is typically done when a neural network is seeing no improvement.\n",
        "\n",
        "**Vocabulary/Concepts**\n",
        "*   Callback............A set of functions that can be executed at any step during training\n",
        "*   EarlyStopping...A callback that tracks the neural network's performance and stops training if it stops improving\n",
        "\n",
        "**Execution**\n",
        "\n",
        "The \"my_callbacks\" object is initialized to be used later during training. By default, Keras's training method, \".fit()\", includes a callback that returns metrics after each epoch/batch the neural network completes. This will help us track the network's improvement during training to determine if it will produce desired results, if it has overfitted, etc.\n",
        "\n",
        "**General**\n",
        "*   Keras training always keeps track of runtime metrics.\n",
        "*   There are various callbacks with different functions like saving a networks's progress, stopping in response to a certain stimuli, streaming the network's progress, and more. \n",
        "\n",
        "**Interpretting Code**\n",
        "*   \"my_callbacks\" is being initialized as a callback object\n",
        "*   \"EarlyStopping()\" constructs the callback object by taking three parameters:\n",
        "  *   \"monitor=\"acc\"\" lets the function know it will monitor accuracy to measure improvement\n",
        "  *   \"patience=5\" tells the function to wait five epochs no improvement before it stops training\n",
        "  *   \"mode=max\" tells the function to stop training when the monitored value stops increasing as opposed to \"mode=min\" which tells the function to stop training when the monitored value stops decreasing\n",
        "  \n",
        "**External Information**\n",
        "*   [Keras Callbacks](https://keras.io/callbacks/)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "WE0fMQElld7w",
        "colab_type": "code",
        "outputId": "07da04a3-ed1f-46c5-f27d-f10049d9376a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "cell_type": "code",
      "source": [
        "#Fit model\n",
        "hist = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.3,\n",
        "                    callbacks=my_callbacks)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/20\n",
            "42000/42000 [==============================] - 8s 184us/step - loss: 0.2924 - acc: 0.9100 - val_loss: 0.1447 - val_acc: 0.9543\n",
            "Epoch 2/20\n",
            "42000/42000 [==============================] - 7s 173us/step - loss: 0.1167 - acc: 0.9648 - val_loss: 0.1157 - val_acc: 0.9658\n",
            "Epoch 3/20\n",
            "42000/42000 [==============================] - 7s 171us/step - loss: 0.0825 - acc: 0.9750 - val_loss: 0.1124 - val_acc: 0.9701\n",
            "Epoch 4/20\n",
            "42000/42000 [==============================] - 7s 171us/step - loss: 0.0636 - acc: 0.9808 - val_loss: 0.1074 - val_acc: 0.9723\n",
            "Epoch 5/20\n",
            "42000/42000 [==============================] - 7s 173us/step - loss: 0.0522 - acc: 0.9843 - val_loss: 0.1091 - val_acc: 0.9724\n",
            "Epoch 6/20\n",
            "42000/42000 [==============================] - 7s 175us/step - loss: 0.0427 - acc: 0.9866 - val_loss: 0.1249 - val_acc: 0.9694\n",
            "Epoch 7/20\n",
            "42000/42000 [==============================] - 7s 177us/step - loss: 0.0360 - acc: 0.9890 - val_loss: 0.1149 - val_acc: 0.9762\n",
            "Epoch 8/20\n",
            "42000/42000 [==============================] - 7s 175us/step - loss: 0.0309 - acc: 0.9904 - val_loss: 0.1245 - val_acc: 0.9743\n",
            "Epoch 9/20\n",
            "42000/42000 [==============================] - 7s 174us/step - loss: 0.0277 - acc: 0.9918 - val_loss: 0.1379 - val_acc: 0.9728\n",
            "Epoch 10/20\n",
            "42000/42000 [==============================] - 7s 175us/step - loss: 0.0255 - acc: 0.9920 - val_loss: 0.1102 - val_acc: 0.9791\n",
            "Epoch 11/20\n",
            "42000/42000 [==============================] - 7s 175us/step - loss: 0.0232 - acc: 0.9930 - val_loss: 0.1163 - val_acc: 0.9793\n",
            "Epoch 12/20\n",
            "42000/42000 [==============================] - 7s 175us/step - loss: 0.0209 - acc: 0.9937 - val_loss: 0.1198 - val_acc: 0.9777\n",
            "Epoch 13/20\n",
            "42000/42000 [==============================] - 7s 173us/step - loss: 0.0209 - acc: 0.9940 - val_loss: 0.1358 - val_acc: 0.9772\n",
            "Epoch 14/20\n",
            "42000/42000 [==============================] - 7s 177us/step - loss: 0.0193 - acc: 0.9944 - val_loss: 0.1311 - val_acc: 0.9787\n",
            "Epoch 15/20\n",
            "42000/42000 [==============================] - 7s 174us/step - loss: 0.0174 - acc: 0.9948 - val_loss: 0.1425 - val_acc: 0.9771\n",
            "Epoch 16/20\n",
            "42000/42000 [==============================] - 7s 174us/step - loss: 0.0171 - acc: 0.9951 - val_loss: 0.1396 - val_acc: 0.9780\n",
            "Epoch 17/20\n",
            "42000/42000 [==============================] - 7s 174us/step - loss: 0.0151 - acc: 0.9954 - val_loss: 0.1345 - val_acc: 0.9798\n",
            "Epoch 18/20\n",
            "42000/42000 [==============================] - 7s 176us/step - loss: 0.0142 - acc: 0.9959 - val_loss: 0.1345 - val_acc: 0.9793\n",
            "Epoch 19/20\n",
            "42000/42000 [==============================] - 7s 175us/step - loss: 0.0150 - acc: 0.9958 - val_loss: 0.1398 - val_acc: 0.9798\n",
            "Epoch 20/20\n",
            "42000/42000 [==============================] - 7s 171us/step - loss: 0.0148 - acc: 0.9959 - val_loss: 0.1444 - val_acc: 0.9786\n",
            "Test loss: 0.12470296922447197\n",
            "Test accuracy: 0.9819\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3fMy8haHleEf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Training & Testing**\n",
        "\n",
        "By now, the neural network can finally be trained and tested. This cell trains the neural network, saves its training history, uses the trained network to evaluate a new set of test data, saves its test score, then prints out the scores for the user to see.\n",
        "\n",
        "**Vocabulary/Concepts**\n",
        "*   Fitting..................Synonym for \"training\"; Trial runs on test data to fuel backpropagation which mathematically analyzes error to reduce future error. Backpropagation occurs after a batch is run through and is the source of a network's \"learning\" capabilities\n",
        "*   Validation Data...Miniature test data that tests the network at the end of each epoch. Validation data comes from splitting training data again. In scholastic terms, training data is homework, validation data is a quiz, and test data is a major test\n",
        "*   Evaluating...........Synonym for \"testing\"; Occurs after training to measure how well the machine has learned. A countermeasure to overfitting\n",
        "\n",
        "**Execution**\n",
        "\n",
        "The first line trains the network and then saves its history in an object called \"hist\". While the network trains, it will return metrics on its performance. In the next line of code, the fully trained model is tested with the test data that was set aside earlier to measure the validity of the network's performance. The outputs of the test is saved to \"score\" which is then printed out for the user to read in the next two lines. This cell, and the next couple of cells, give the user data to judge the network.\n",
        "\n",
        "**General**\n",
        "*   Evaluating is suppose to test your trained network, but for comparison, you can evaluate your network before you train it to compare the untrained network to the trained network. Just be sure that it isn't working on real-world data that carry consequences for mistakes.\n",
        "*   It's always wise to save the network's history and test score to view later, especially for hyperparameter optimization.\n",
        "\n",
        "**Interpretting Code**\n",
        "*   \"hist\" is a object that's initialized as the history returned by the \".fit()\" method\n",
        "*   \"model.fit()\" is a method from Keras that trains the neural network. It's taking seven parameters:\n",
        "  *   \"x_train\" refers to the training images that will be input into the network\n",
        "  *   \"y_train\" refers to the training labels that will be compared to the network's output\n",
        "  *   \"batch_size=batch_size\" sets the batch size for Keras's training equal to our batch size that we initialized earlier\n",
        "  *   \"epochs=epochs\" sets the epochs for Keras's training equal to our epochs that we also initialized earlier\n",
        "  *   \"verbose=1\" tells Keras how verbose it should be regarding the network's training progress. \"0\" tells Keras to stay silent. \"1\" tells Keras to show a progress bar. \"2\" tells Keras to show a bar per epoch\n",
        "  *   \"validation_split=0.3\" tells Keras to split training data once again into training data and validation data (with corresponding labels). It's exactly like the test_train_split() from sklearn, except this happens inside training\n",
        "  *   \"callbacks=my_callbacks\" sets the callbacks for Keras's training equal to the callbacks we initialized earlier\n",
        "*   \"score\" is a tuple that's initialized to hold the accuracy and loss of the model which is returned by the \".evaluate()\" method\n",
        "*   \"model.evaluate()\" is a method from Keras that tests the neural network. It's taking three parameters:\n",
        "  *   \"x_test\" refers to the test images that were set aside earlier. These will be fed into our trained neural network to judge its performance\n",
        "  *   \"y_test\" refers to the test labels that were set aside earlier. These will also be fed into our trained neural network to judge its performance\n",
        "  *   \"verbose=0\" tells Keras to stay silent while evaluating.\n",
        "*   \"print('Testing Loss:', score[0])\" prints the loss of the model from the testing session which is stored as the first element of the \"score\" tuple\n",
        "*   \"print('Testing Accuracy:', score[1])\" prints the accuracy of the model from the testing session which is stored as the second element of the \"score\" tuple\n",
        "\n",
        "**External Information**\n",
        "*   [Testing, Training, and Validation Data](https://stackoverflow.com/questions/2976452/whats-is-the-difference-between-train-validation-and-test-set-in-neural-netwo)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "xH4MTWMvliDM",
        "colab_type": "code",
        "outputId": "ea5c5612-1015-4c6d-d5de-bb5975e203d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "#Summarize Model\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 669,706\n",
            "Trainable params: 669,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XCq7EkOWliPZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Neural Network Summary**\n",
        "\n",
        "This cell gives a summary of the model that tells the user the shape of the data as it moves through the neural network and the qualities of each layer's neuron parameters which are the neuron's weights and biases.\n",
        "\n",
        "**Vocabulary/Concepts**\n",
        "*   Neuron Parameters...Refers the to weights and biases in a neuron which can be altered by backpropagation but some neuron parameters can be trained and others not\n",
        "\n",
        "**Execution**\n",
        "\n",
        "This line of code simple describes the model the user has built in more depth using ther \".summary()\" method from Keras. This is useful for keeping track of what layers are being used, the output shape of each layer, and the number of neuron parameters each layer has that can be edited.\n",
        "\n",
        "**General**\n",
        "*   Outputs from layer to layer are expected to decrease since classification problems typically classify data into a small number of categories based on a large amount of input data.\n",
        "*   Not every layer has revisable parameters. Instead, they carry out an operation on the outputs of the previous layer.\n",
        "*   Dense layers will typically have the most neuron parameters and account for most of the data manipulation.\n",
        "\n",
        "**Interpretting Code**\n",
        "*   \"model.summary()\" prints out a description of the neural network model that the user has created which quickly calculates the number of neuron parameters in each layer and displays the output shape of each layer. The \".summary()\" method is a useful tool for quick information about a network\n",
        "\n",
        "**External Sources**\n",
        "*   [Neuron Structure](https://www.neuraldesigner.com/blog/perceptron-the-main-component-of-neural-networks)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "G0hc_d16l1AX",
        "colab_type": "code",
        "outputId": "2e36faff-ba4a-4d43-c5ad-2e1590d71ce5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "cell_type": "code",
      "source": [
        "#Plotting training accuracy & validation accuracy\n",
        "epoch_list = list(range(1, len(hist.history['acc']) + 1))\n",
        "plt.plot(epoch_list, hist.history['acc'], epoch_list, hist.history['val_acc'])\n",
        "plt.legend((\"Training Accuracy\", \"Validation Accuracy\"))\n",
        "plt.show"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFNCAYAAAA6iqfcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd853f99f98>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "_igZgQjYl1JH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Visualizing Metrics**\n",
        "\n",
        "This cell creates a graph to visualize the development of our network over each epoch with the help of a library called Matplotlib.\n",
        "\n",
        "**Vocabulary/Concepts**\n",
        "*   Convergence...............When the changes of the metrics in a neural network (like loss or accuracy) approach zero, the network is \"converging\". A network reaches convergence when the metrics start to plateau over epochs; they begin to reach a limit\n",
        "*   Training Accuracy.......How accurate the network is at classifying training data\n",
        "*   Validation Accuracy....How accurate the network is at classifying validation data\n",
        "\n",
        "**Execution**\n",
        "\n",
        "The first line of code initializes \"epoch_list\" as a list of numbers starting from 1 to the total number of epochs plus one. Second, data is stored on a chart using Matplotlib where the training accuracy and validation accuracy are plotted with respect to the number of epochs. Then, a legend is created on the chart which denotes the dependent variables: \"Training Accuracy\" and \"Validation Accuracy\". Lastly, the plot is shown. This tool, like the summary and callbacks, are useful in comparing neural networks for hyperparameter optimization or judging a network by visualizing metrics.\n",
        "\n",
        "**General**\n",
        "*   When plotting, it is nice to include a legend and title so readers can understand what data is being plotted.\n",
        "*   \"plt.show\" is necessary to display a graphic of the chart. From there, the chart can be saved as an image, exported, printed, etc.\n",
        "\n",
        "**Interpretting Code**\n",
        "*   \"epoch_list\" is a list that will store epoch numbers as individual elements\n",
        "*   \"list()\" creates a list of elements. The elements are specified by \"range(1, len(hist.history['acc']) + 1))\". Essentially, every integer inclusively between 1 and the length of the \"hist.history['acc']\" array plus one (which should be 50) becomes an element in the list\n",
        "*   \"plt.plot()\" is a Matplotlib method that stores data in a plot. It's taking four parameters that come in pairs:\n",
        "  *   \"epoch_list\" is the list that was initialized earlier. This series of integers will serve as the x axis of the plot for both \"hist.history['acc']\" and \"hist.history['val_acc']\" which is why \"epoch_list\" appears twice\n",
        "  *   \"hist.history['acc']\" is an array that stores the accuracy of the neural network from epoch to epoch. This is a dependent variable that will be plotted on the y axis\n",
        "  *   \"hist.history['val_acc']\" is an array that stores the validation accuracy of the neural network from epoch to epoch. This is also a dependent variable that will be plotted on the y axis\n",
        "*   \"plt.legend()\" is a Matplotlib method that creates a legend on the chart. It's taking one parameter:\n",
        "  *   \"(\"Training Accuracy\", \"Validation Accuracy\")\" is a tuple whose two elements refer to the previously plotted pairs. The first element of the tuple tells Matplotlib that the first pair of independent and dependent variables are measuring training accuracy. The second element tells Matplotlib that the second pair of variables measure validation accuracy.\n",
        "*   \"plt.show\" is Matplotlib method that is necessary for showing the chart the user created beforehand. From there, the chart can be saved as an image, printed, exported, etc.\n",
        "\n",
        "**External Information**\n",
        "*   [Convergent Series](https://en.wikipedia.org/wiki/Convergent_series)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "qrkympHQBCgc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Entire Program\n",
        "\n",
        "#Import necessary libraries\n",
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "\n",
        "#Suppress Warnings (Optional)\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\"\n",
        "\n",
        "#Set Variables\n",
        "epochs = 20\n",
        "num_classes = 10\n",
        "batch_size = 128\n",
        "\n",
        "#Load and separate data into X: Features, Y: Labels\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "#Data formatting\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "#Notes\n",
        "#*Not converting array types into floats increases loss\n",
        "#*Division seems to have no effect\n",
        "#*Are astype, reshape, and shape from numpy? There is no import for numpy\n",
        "\n",
        "# convert class vectors to binary class matrices (one-hot encoding)\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "#Defining the model (Hyper- opt: hyperparameter optimization) Topology\n",
        "# Hyperparams: Kernal size, node amounts, max pool size\n",
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "#Compile Model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Setting Early Stopping\n",
        "my_callbacks = [EarlyStopping(monitor=\"acc\", patience=5, mode=max)]\n",
        "\n",
        "#Fit model\n",
        "hist = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.3\n",
        "                    callbacks=my_callbacks)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "#Summarize Model\n",
        "model.summary()\n",
        "\n",
        "#Plotting training accuracy & validation accuracy\n",
        "epoch_list = list(range(1, len(hist.history['acc']) + 1))\n",
        "plt.plot(epoch_list, hist.history['acc'], epoch_list, hist.history['val_acc'])\n",
        "plt.legend((\"Training Accuracy\", \"Validation Accuracy\"))\n",
        "plt.show"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}